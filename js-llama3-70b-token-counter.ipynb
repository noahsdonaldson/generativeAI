{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1baf5187-37d8-43b9-ba9e-ed6c6f053922",
   "metadata": {},
   "source": [
    "# Download Model\n",
    "### Make sure you have enough drive space to get this model (300GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958e060-8449-427d-960f-589a8221ee66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1de574-9fbd-4549-b49e-ba1e06cf64cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_token = \"\" # Fill in your token here\n",
    "hf_model_id = \"meta-llama/Llama-3.1-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de30dca-c905-4533-aec2-043548a5e68b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "models_path = Path.home().joinpath('SageMaker', hf_model_id)\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = snapshot_download(repo_id=hf_model_id, local_dir=models_path, token=access_token)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64db10c-a14a-417d-b850-c526ae35fcf0",
   "metadata": {},
   "source": [
    "# Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46395237-8595-4859-bd11-7d86a0501a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e238265-ae71-4ec7-a1cb-237cb2132d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(f\"/home/ec2-user/SageMaker/{hf_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d3e28-860d-4ac6-9a11-367e41357bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tokenizer.tokenize(\"what is sagemaker?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25127add-846a-4335-b650-7be593451c83",
   "metadata": {},
   "source": [
    "# Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a37ba-1d97-4aa4-9cfa-d5248068591e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from datetime import datetime\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(f\"llama-js\")\n",
    "\n",
    "instance_type = \"ml.p4d.24xlarge\" # This can also be \"ml.g5.48xlarge\", but will run slower than the p4d version\n",
    "model_id = \"meta-textgeneration-llama-3-1-70b-instruct\"\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, instance_type=instance_type, env={'SERVING_CHUNKED_READ_TIMEOUT': str(600),'SERVING_PREDICT_TIMEOUT': str(600)})\n",
    "predictor = model.deploy(endpoint_name=endpoint_name, accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012da226-cf9e-4fac-9d81-407046470872",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e865f2-2669-4d1a-8246-e62f944c8e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function was written by Generative AI to create randomly long strings to simulate large token payloads\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_word(min_length=3, max_length=10):\n",
    "    length = random.randint(min_length, max_length)\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\n",
    "\n",
    "def generate_text(num_words):\n",
    "    words = []\n",
    "    for _ in range(num_words):\n",
    "        words.append(generate_word())\n",
    "\n",
    "        # Add punctuation occasionally\n",
    "        if random.random() < 0.1:\n",
    "            words[-1] += random.choice('.,...?!')\n",
    "\n",
    "        # Add paragraph break occasionally\n",
    "        if random.random() < 0.05:\n",
    "            words.append('\\n\\n')\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1221f-8d8b-4626-9205-4233cd4cb9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "import botocore\n",
    "import boto3\n",
    "config = botocore.config.Config(\n",
    "    read_timeout=600,\n",
    "    connect_timeout=600\n",
    ")\n",
    "\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", verify=True, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18beba1d-8fff-48ae-b4cd-3320dd8e4508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload_text = generate_text(30000) # This seems to produce around 110000 tokens\n",
    "input_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for travel tips and recommendations {payload_text}<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Write a movie screenplay about corgis being attacked by aliens<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>It was a dark night\"\"\"\n",
    "print(f\"# tokens = {len(tokenizer.tokenize(input_prompt))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cf1a8-c71e-486d-bcdb-de4bfa6ea19f",
   "metadata": {},
   "source": [
    "## Non-Streaming Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd400ecb-1441-45ee-bf50-281090c189a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Start Time - {datetime.now().strftime('%H:%M:%S')}\")       # Hours:Minutes:Seconds\n",
    "payload = {\n",
    "    \"inputs\": input_prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 4096,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "    },\n",
    "}\n",
    "# print(payload)\n",
    "try:\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=f\"{endpoint_name}\",\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    # print(response)\n",
    "    t = response['Body']\n",
    "    # print(t)\n",
    "    t_read = t.read()\n",
    "    # print(t_read)\n",
    "    j = json.loads(t_read)\n",
    "    print(j['generated_text'])\n",
    "except Exception as e:\n",
    "    print(f\"Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ab241-5772-46ca-a234-c346508eac06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Start Time - {datetime.now().strftime('%H:%M:%S')}\")       # Hours:Minutes:Seconds\n",
    "payload = {\n",
    "    \"inputs\": input_prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\":4096, \n",
    "        \"top_p\":0.9, \n",
    "        \"temperature\":0.6, \n",
    "        \"stream\": True\n",
    "    }\n",
    "}\n",
    "# print(payload)\n",
    "try:\n",
    "    streaming_response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=f\"{endpoint_name}\",\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    chunk = ''\n",
    "    first_token = False\n",
    "    for event in streaming_response[\"Body\"]:\n",
    "        # print(event)\n",
    "        chunk += event[\"PayloadPart\"][\"Bytes\"].decode('utf-8')\n",
    "        try:\n",
    "            chunk_dict = json.loads(chunk)\n",
    "            if not first_token:\n",
    "                first_token = True\n",
    "                print(f\"Time of First Token: {datetime.now().strftime('%H:%M:%S')}\")       # Hours:Minutes:Seconds\n",
    "            chunk = ''\n",
    "            print(chunk_dict['token'].get(\"text\", \"\"), end=\"\")\n",
    "        except Exception as e:\n",
    "            None\n",
    "except Exception as e:\n",
    "    print(f\"Exception - {e}\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97867f-9512-4988-97e9-cba3c62d0dd8",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6ca27-9d4f-4cb4-8585-68cfbd2476a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf63180-353f-4d41-a9e6-66e42593e0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
